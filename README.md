# ğŸ”Š Shadow Sound

> Your phone, listening so you don't have to.

Shadow Sound translates dangerous sounds â€” sirens, horns, shouting, glass breaking â€” into haptic vibration patterns for deaf and hard-of-hearing users. No extra hardware. Just the phone you already have.

Built at **HackED 2026** in 24 hours.

---

## How It Works

```
Phone mic (1.5s chunks)
  â””â”€â”€ WebSocket â†’ Python backend
        â””â”€â”€ YAMNet ML model
              â””â”€â”€ sound_type + urgency + haptic_pattern
                    â””â”€â”€ haptic feedback + screen flash
```

1. App captures audio in 1.5-second chunks via the device mic
2. Chunks are base64-encoded and streamed to the backend over WebSocket
3. YAMNet classifies the audio across 521 AudioSet event classes
4. Backend maps relevant classes to 14 safety categories and returns a detection
5. App fires a distinct haptic pattern and flashes the screen

End-to-end latency: **< 2 seconds**

---

## Sound Categories

| Sound | Haptic Pattern | Urgency |
|-------|---------------|---------|
| Emergency siren | Rapid pulse Ã— 3 | ğŸ”´ Critical |
| Glass breaking | Rapid pulse Ã— 3 | ğŸ”´ Critical |
| Car horn | Double tap | ğŸŸ  High |
| Vehicle approaching | Double tap | ğŸŸ  High |
| Tire screech | Rapid pulse Ã— 3 | ğŸŸ  High |
| Shouting / yelling | Long buzz | ğŸŸ¡ Medium |
| Bicycle bell | Quick triple tap | ğŸŸ¡ Medium |
| Car alarm | Rapid pulse Ã— 3 | ğŸŸ¡ Medium |
| Train | Double tap | ğŸŸ¡ Medium |
| Dog barking | Gentle pulse | ğŸŸ¢ Low |
| Footsteps running | Gentle pulse | ğŸŸ¢ Low |
| Construction noise | Gentle pulse | ğŸŸ¢ Low |
| Door slam | Single tap | ğŸŸ¢ Low |
| Aircraft | Gentle pulse | ğŸŸ¢ Low |

---

## Stack

**Frontend**
- React Native (Expo) â€” iOS + Android
- `expo-av` â€” audio capture
- `expo-haptics` â€” haptic feedback
- `expo-router` â€” navigation
- TypeScript throughout

**Backend**
- Python 3.12 + FastAPI
- WebSocket endpoint (`/ws/audio`)
- YAMNet via TensorFlow Hub â€” pre-trained audio classification
- pydub + librosa â€” audio decoding and resampling
- Docker â€” containerized, hardened, non-root

---

## Repo Structure

```
shadow-sound/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â””â”€â”€ (tabs)/
â”‚   â”‚       â”œâ”€â”€ index.tsx          # Home â€” listening + alerts
â”‚   â”‚       â”œâ”€â”€ explore.tsx        # Explore tab
â”‚   â”‚       â””â”€â”€ profile.tsx        # Detection history
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ ui/
â”‚   â”‚       â”œâ”€â”€ AlertFlash.tsx     # Full-screen urgency flash
â”‚   â”‚       â”œâ”€â”€ StatusIndicator.tsx
â”‚   â”‚       â”œâ”€â”€ PermissionGate.tsx
â”‚   â”‚       â””â”€â”€ Button.tsx
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useSoundDetection.ts   # Main pipeline hook
â”‚   â”‚   â””â”€â”€ useAudioCapture.ts     # Mic capture hook
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ haptic.service.ts      # Haptic pattern definitions
â”‚   â””â”€â”€ src/services/
â”‚       â”œâ”€â”€ audio-capture.service.ts  # Audio recording loop
â”‚       â””â”€â”€ websocket.service.ts      # WS connection + auth
â””â”€â”€ backend/
    â”œâ”€â”€ main.py                    # FastAPI app + WebSocket endpoint
    â”œâ”€â”€ classifier.py              # YAMNet wrapper
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ Dockerfile
```

---

## Getting Started

### Prerequisites

- Node.js 18+
- Python 3.12+
- Docker (for backend)
- Expo Go app on a physical device (simulators don't vibrate)

### Backend

```bash
# With Docker (recommended)
docker compose up

# Or locally
cd backend
pip install -r requirements.txt
uvicorn main:app --host 0.0.0.0 --port 8000
```

The first run downloads the YAMNet model (~25MB) from TensorFlow Hub. Subsequent runs use the cached version.

### Frontend

```bash
cd frontend
npm install
npm start
```

Scan the QR code with Expo Go. **Test on a real device** â€” haptics don't work in simulators.

### Connecting to your backend

Update the WebSocket URL in `frontend/src/services/websocket.service.ts`:

```typescript
const DEFAULT_WS_URL = "ws://<your-machine-ip>:8000/ws/audio";
```

Use your local network IP (not `localhost`) when testing on a physical device.

### Mock mode

The backend supports mock detections for frontend development without a real ML model:

```
ws://localhost:8000/ws/audio?mock=true
```

Or use the **âš¡ Demo Mode** button in the app to fire test alerts instantly without any backend connection.

---

## API

### WebSocket `/ws/audio`

**Auth handshake** (client â†’ server):
```json
{ "type": "auth", "device_id": "ios-abc123", "api_version": "1.0" }
```

**Audio chunk** (client â†’ server):
```json
{ "type": "audio_chunk", "audio_data": "<base64>", "sample_rate": 16000 }
```

**Detection** (server â†’ client):
```json
{
  "type": "detection",
  "timestamp": 1740000000,
  "detections": [{
    "sound_type": "emergency_siren",
    "confidence": 0.91,
    "urgency": "critical",
    "haptic_pattern": "rapid_pulse_3x"
  }],
  "processing_time_ms": 340
}
```

### REST

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Health check |
| GET | `/api/v1/settings/{device_id}` | Get device settings |
| PUT | `/api/v1/settings/{device_id}` | Update device settings |
| POST | `/api/v1/feedback` | Submit detection feedback |

---

## Permissions

**iOS** â€” add to `Info.plist`:
```xml
<key>NSMicrophoneUsageDescription</key>
<string>Shadow Sound listens for safety sounds around you.</string>
```

**Android** â€” add to `AndroidManifest.xml`:
```xml
<uses-permission android:name="android.permission.RECORD_AUDIO"/>
<uses-permission android:name="android.permission.VIBRATE"/>
```

Both are already configured in `app.json`.

---

## Team

Built by **Chirayu Shah** at HackED 2026 â€” February 21â€“22, 2026.

---

## What's Next

- âŒš Wear OS / Apple Watch companion app
- ğŸ§­ Directional detection â€” compass showing where the sound came from
- ğŸ“± Always-on background mode
- ğŸ§  On-device inference with TensorFlow Lite (no backend required)
- ğŸšï¸ Personalized sensitivity per sound category